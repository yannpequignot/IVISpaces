{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yann/IVISpaces\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.sgd import SGD\n",
    "\n",
    "import math\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "from Metrics import evaluate_metrics\n",
    "from Experiments import get_setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        \n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.to(device)\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_gaussian_loss(output, target, sigma, no_dim=1):\n",
    "    exponent = -0.5*(target - output)**2/sigma**2\n",
    "    log_coeff = -no_dim*torch.log(sigma*torch.tensor(2*math.pi).sqrt())\n",
    "    \n",
    "    return -(log_coeff + exponent).sum()\n",
    "\n",
    "\n",
    "def get_kl_divergence(weights, prior, varpost):\n",
    "    prior_loglik = prior.loglik(weights)\n",
    "    \n",
    "    varpost_loglik = varpost.loglik(weights)\n",
    "    varpost_lik = varpost_loglik.exp()\n",
    "    \n",
    "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
    "\n",
    "\n",
    "class gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def loglik(self, weights):\n",
    "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
    "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
    "        \n",
    "        return (exponent + log_coeff).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_Dropout_Layer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_prob):\n",
    "        super(MC_Dropout_Layer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        self.weights = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
    "        self.biases = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        dropout_mask = torch.bernoulli((1 - self.dropout_prob)*torch.ones(self.weights.shape)).to(device)\n",
    "        \n",
    "        return torch.mm(x, self.weights*dropout_mask) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_Dropout_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, no_units, init_log_noise, drop_prob):\n",
    "        super(MC_Dropout_Model, self).__init__()\n",
    "        \n",
    "        self.drop_prob=drop_prob\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, no_units)\n",
    "        self.layer2 = nn.Linear(no_units, output_dim)\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "        self.log_noise = nn.Parameter(torch.FloatTensor([init_log_noise]).to(device))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(-1, self.input_dim)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = F.dropout(x, p=drop_prob, training=True)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_Dropout_Wrapper:\n",
    "    def __init__(self, input_dim, output_dim, no_units, learn_rate, batch_size, no_batches, weight_decay, init_log_noise):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.no_batches = no_batches\n",
    "        \n",
    "        self.network = MC_Dropout_Model(input_dim = input_dim, output_dim = output_dim,\n",
    "                                        no_units = no_units, init_log_noise = init_log_noise, drop_prob = drop_prob)\n",
    "        self.network.to(device)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "        self.loss_func = log_gaussian_loss\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        x, y = to_variable(var=(x, y), cuda=True)\n",
    "        \n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        output = self.network(x)\n",
    "        loss = self.loss_func(output, y, torch.exp(self.network.log_noise), 1)/len(x)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def get_loss_and_rmse(self, x, y, num_samples):\n",
    "        x, y = to_variable(var=(x, y), cuda=True)\n",
    "        \n",
    "        means, stds = [], []\n",
    "        for i in range(num_samples):\n",
    "            output = self.network(x)\n",
    "            means.append(output)\n",
    "        \n",
    "        means = torch.cat(means, dim=1)\n",
    "        mean = means.mean(dim=-1)[:, None]\n",
    "        std = ((means.var(dim=-1) + torch.exp(self.network.log_noise)**2)**0.5)[:, None]\n",
    "        loss = self.loss_func(mean, y, std, 1)\n",
    "        \n",
    "        rmse = ((mean - y)**2).mean()**0.5\n",
    "\n",
    "        return loss.detach().cpu(), rmse.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_test(x_train,x_test,y_train,number_point):\n",
    "    np.random.seed(2)\n",
    "    no_points = number_point\n",
    "    lengthscale = 1\n",
    "    variance = 1.0 \n",
    "    sig_noise = 0.3\n",
    "    x = torch.cat((x_train,x_test),0).cpu().numpy()\n",
    "    x.sort(axis = 0)\n",
    "    \n",
    "    num_epochs, batch_size = 2000, len(x_train)\n",
    "    \n",
    "    net = MC_Dropout_Wrapper(input_dim = 1, output_dim=1, no_units=200, learn_rate=1e-2,\n",
    "                         batch_size=batch_size, no_batches=1, init_log_noise=0, weight_decay=1e-2)\n",
    "    fit_loss_train = np.zeros(num_epochs)\n",
    "    best_net, best_loss = None, float('inf')\n",
    "    nets, losses = [], []\n",
    "    for i in range(num_epochs):\n",
    "        loss = net.fit(x_train, y_train)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print('Epoch: %4d, Train loss = %7.3f' % (i, loss.cpu().data.numpy()/batch_size))\n",
    "\n",
    "    return net, losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph():\n",
    "    samples = []\n",
    "    noises = []\n",
    "    for i in range(1000):\n",
    "        preds = net.network.forward(torch.linspace(-2, 2, 200).to(device)).cpu().data.numpy()\n",
    "        samples.append(preds)\n",
    "    \n",
    "    samples = np.array(samples)\n",
    "    means = (samples.mean(axis = 0)).reshape(-1)\n",
    "\n",
    "    aleatoric = torch.exp(net.network.log_noise).cpu().data.numpy()\n",
    "    epistemic = (samples.var(axis = 0)**0.5).reshape(-1)\n",
    "    total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
    "\n",
    "\n",
    "    c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "         '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "    plt.figure(figsize = (6, 5))\n",
    "    plt.style.use('default')\n",
    "    plt.scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
    "    plt.fill_between(np.linspace(-2, 2, 200), means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
    "    plt.fill_between(np.linspace(-2, 2, 200), means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
    "    plt.fill_between(np.linspace(-2, 2, 200), means - aleatoric, means + aleatoric, color = c[1], alpha = 0.4, label = 'Aleatoric')\n",
    "    plt.plot(np.linspace(-2, 2, 200), means, color = 'black', linewidth = 1)\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-5, 7])\n",
    "    plt.xlabel('$x$', fontsize=30)\n",
    "    plt.title('MC dropout', fontsize=40)\n",
    "    plt.tick_params(labelsize=10)\n",
    "    plt.xticks(np.arange(-2, 2.1, 1))\n",
    "    plt.yticks(np.arange(-4, 7, 2))\n",
    "    plt.gca().set_yticklabels([])\n",
    "    plt.gca().yaxis.grid(alpha=0.3)\n",
    "    plt.gca().xaxis.grid(alpha=0.3)\n",
    "    plt.savefig('mc_dropout_hetero.pdf', bbox_inches = 'tight')\n",
    "\n",
    "    #files.download(\"mc_dropout_hetero.pdf\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI Dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mc_dropout(x_train,y_train,x_test,y_test, y_stds, drop_prob, num_epochs, num_units, learn_rate, weight_decay, log_every, num_samples):\n",
    "    in_dim = x_train.shape[1]\n",
    "    out_dim = y_train.shape[1]\n",
    "    train_logliks, test_logliks = [], []\n",
    "    train_rmses, test_rmses = [], []\n",
    "    \n",
    "    net = MC_Dropout_Wrapper(input_dim=in_dim, output_dim=out_dim, no_units=num_units,learn_rate=learn_rate, batch_size=batch_size, no_batches=1, init_log_noise=0, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    losses = []\n",
    "    fit_loss_train = np.zeros(num_epochs)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        loss = net.fit(x_train, y_train)\n",
    "        losses.append(loss)\n",
    "                \n",
    "        if i % log_every == 0 or i == num_epochs - 1:\n",
    "            test_loss, rmse = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "            test_loss, rmse = test_loss.cpu().data.numpy(), rmse.cpu().data.numpy()\n",
    "\n",
    "            print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f RMSE: %.3f' %\n",
    "                    (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), rmse*y_stds[0].cpu().data.numpy()))\n",
    "\n",
    "\n",
    "    train_loss, train_rmse = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "    test_loss, test_rmse = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "    y_stds=y_stds.cpu()    \n",
    "    train_logliks.append((train_loss.cpu().data.numpy()/len(x_train) + np.log(y_stds)[0]))\n",
    "    test_logliks.append((test_loss.cpu().data.numpy()/len(x_test) + np.log(y_stds)[0]))\n",
    "\n",
    "    train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
    "    test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
    "        \n",
    "\n",
    "\n",
    "    print('Train log. lik. = %6.3f +/- %6.3f' % (-np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
    "    print('Test  log. lik. = %6.3f +/- %6.3f' % (-np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
    "    print('Train RMSE      = %6.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
    "    print('Test  RMSE      = %6.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
    "    \n",
    "    return net , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Toy#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim: 1\n",
      "output dim: 1\n",
      "nb of train samples: 120\n"
     ]
    }
   ],
   "source": [
    "drop_prob=0.05\n",
    "\n",
    "\n",
    "#toy datasets : ['foong','foong_sparse', 'foong_mixed','foong2d']\n",
    "\n",
    "dataset='foong' \n",
    "\n",
    "setup_ = get_setup(dataset) #get a module\n",
    "setup=setup_.Setup(device) #get an object\n",
    "\n",
    "x_train, y_train=setup.train_data() #scaled_data\n",
    "x_test, y_test=setup.test_data()\n",
    "input_dim=x_train.shape[1]\n",
    "output_dim=y_train.shape[1]\n",
    "print('input dim: {}'.format(input_dim))\n",
    "print('output dim: {}'.format(output_dim))\n",
    "print('nb of train samples: {}'.format(len(x_train)))\n",
    "\n",
    "#scalar used to scale the train target to have std=1\n",
    "std_y_train = torch.tensor(1.)\n",
    "if hasattr(setup, '_scaler_y'):\n",
    "    print('scaler target: {}'.format(setup._scaler_y.scale_.item()))\n",
    "    std_y_train=torch.tensor(setup._scaler_y.scale_, device=device).squeeze().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sqrt(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-bae7eed5468e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoy_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-368f4f63aa72>\u001b[0m in \u001b[0;36mtoy_test\u001b[0;34m(x_train, x_test, y_train, number_point)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mnets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-3d2a02745d23>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-49b7efe9d144>\u001b[0m in \u001b[0;36mlog_gaussian_loss\u001b[0;34m(output, target, sigma, no_dim)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlog_gaussian_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mexponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlog_coeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mno_dim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_coeff\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sqrt(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "net,losses=toy_test(x_train=x_train, x_test=x_test, y_train=y_train,number_point=len(x_train)+len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 13]' is invalid for input of size 200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-cdd83e29cf16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-9cfb7f238d56>\u001b[0m in \u001b[0;36mshow_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnoises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f3dbab0c59c9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 13]' is invalid for input of size 200"
     ]
    }
   ],
   "source": [
    "show_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4e5845dfd0>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.arange(0,len(losses),1),torch.as_tensor(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup(UCI) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim: 13\n",
      "output dim: 1\n",
      "nb of train samples: 455\n",
      "scaler target: 9.307224685633784\n"
     ]
    }
   ],
   "source": [
    "#UCI datasets: ['boston','concrete', 'energy', 'powerplant',  'wine', 'yacht', 'protein', 'navalC'] \n",
    "dataset='boston' \n",
    "\n",
    "setup_ = get_setup(dataset) #get a module\n",
    "setup=setup_.Setup(device) #get an object\n",
    "\n",
    "x_train, y_train=setup.train_data() #scaled_data\n",
    "x_test, y_test=setup.test_data()\n",
    "input_dim=x_train.shape[1]\n",
    "output_dim=y_train.shape[1]\n",
    "print('input dim: {}'.format(input_dim))\n",
    "print('output dim: {}'.format(output_dim))\n",
    "print('nb of train samples: {}'.format(len(x_train)))\n",
    "\n",
    "#scalar used to scale the train target to have std=1\n",
    "std_y_train = torch.tensor(1.)\n",
    "if hasattr(setup, '_scaler_y'):\n",
    "    print('scaler target: {}'.format(setup._scaler_y.scale_.item()))\n",
    "    std_y_train=torch.tensor(setup._scaler_y.scale_, device=device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.00444554224474387\n"
     ]
    }
   ],
   "source": [
    "#predictive model\n",
    "layerwidth=50\n",
    "activation=nn.ReLU()\n",
    "drop_prob=0.05\n",
    "num_epochs=20000\n",
    "batch_size = len(x_train)\n",
    "learn_rate=1e-3 #1e-5 for powerplant , navalC and 1e-6 for protein\n",
    "weight_decay= 1e-1/(len(x_train)+len(x_test))**0.5 #5e-7    #\n",
    "num_samples=50\n",
    "log_every=50\n",
    "print(learn_rate,weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0, Train loss:  0.003 Test loss:  1.248 RMSE: 7.553\n",
      "Epoch:   50, Train loss:  0.002 Test loss:  0.988 RMSE: 4.415\n",
      "Epoch:  100, Train loss:  0.002 Test loss:  0.880 RMSE: 3.350\n",
      "Epoch:  150, Train loss:  0.002 Test loss:  0.808 RMSE: 2.952\n",
      "Epoch:  200, Train loss:  0.002 Test loss:  0.751 RMSE: 2.816\n",
      "Epoch:  250, Train loss:  0.002 Test loss:  0.701 RMSE: 2.780\n",
      "Epoch:  300, Train loss:  0.001 Test loss:  0.645 RMSE: 2.596\n",
      "Epoch:  350, Train loss:  0.001 Test loss:  0.596 RMSE: 2.534\n",
      "Epoch:  400, Train loss:  0.001 Test loss:  0.552 RMSE: 2.518\n",
      "Epoch:  450, Train loss:  0.001 Test loss:  0.512 RMSE: 2.529\n",
      "Epoch:  500, Train loss:  0.001 Test loss:  0.468 RMSE: 2.484\n",
      "Epoch:  550, Train loss:  0.001 Test loss:  0.433 RMSE: 2.518\n",
      "Epoch:  600, Train loss:  0.001 Test loss:  0.403 RMSE: 2.577\n",
      "Epoch:  650, Train loss:  0.001 Test loss:  0.369 RMSE: 2.559\n",
      "Epoch:  700, Train loss:  0.001 Test loss:  0.340 RMSE: 2.583\n",
      "Epoch:  750, Train loss:  0.001 Test loss:  0.311 RMSE: 2.581\n",
      "Epoch:  800, Train loss:  0.000 Test loss:  0.286 RMSE: 2.594\n",
      "Epoch:  850, Train loss:  0.000 Test loss:  0.276 RMSE: 2.671\n",
      "Epoch:  900, Train loss:  0.000 Test loss:  0.245 RMSE: 2.634\n",
      "Epoch:  950, Train loss:  0.000 Test loss:  0.230 RMSE: 2.661\n",
      "Epoch: 1000, Train loss:  0.000 Test loss:  0.213 RMSE: 2.653\n",
      "Epoch: 1050, Train loss:  0.000 Test loss:  0.177 RMSE: 2.583\n",
      "Epoch: 1100, Train loss:  0.000 Test loss:  0.174 RMSE: 2.626\n",
      "Epoch: 1150, Train loss:  0.000 Test loss:  0.136 RMSE: 2.537\n",
      "Epoch: 1200, Train loss: -0.000 Test loss:  0.159 RMSE: 2.632\n",
      "Epoch: 1250, Train loss: -0.000 Test loss:  0.165 RMSE: 2.675\n",
      "Epoch: 1300, Train loss: -0.000 Test loss:  0.142 RMSE: 2.617\n",
      "Epoch: 1350, Train loss: -0.000 Test loss:  0.110 RMSE: 2.553\n",
      "Epoch: 1400, Train loss: -0.000 Test loss:  0.107 RMSE: 2.548\n",
      "Epoch: 1450, Train loss: -0.000 Test loss:  0.125 RMSE: 2.598\n",
      "Epoch: 1500, Train loss: -0.000 Test loss:  0.124 RMSE: 2.623\n",
      "Epoch: 1550, Train loss: -0.000 Test loss:  0.105 RMSE: 2.575\n",
      "Epoch: 1600, Train loss: -0.000 Test loss:  0.119 RMSE: 2.584\n",
      "Epoch: 1650, Train loss: -0.000 Test loss:  0.128 RMSE: 2.598\n",
      "Epoch: 1700, Train loss: -0.000 Test loss:  0.108 RMSE: 2.577\n",
      "Epoch: 1750, Train loss: -0.000 Test loss:  0.110 RMSE: 2.607\n",
      "Epoch: 1800, Train loss: -0.000 Test loss:  0.110 RMSE: 2.551\n",
      "Epoch: 1850, Train loss: -0.000 Test loss:  0.084 RMSE: 2.556\n",
      "Epoch: 1900, Train loss: -0.000 Test loss:  0.083 RMSE: 2.590\n",
      "Epoch: 1950, Train loss: -0.000 Test loss:  0.060 RMSE: 2.513\n",
      "Epoch: 2000, Train loss: -0.000 Test loss:  0.115 RMSE: 2.537\n",
      "Epoch: 2050, Train loss: -0.000 Test loss:  0.042 RMSE: 2.484\n",
      "Epoch: 2100, Train loss: -0.000 Test loss:  0.057 RMSE: 2.428\n",
      "Epoch: 2150, Train loss: -0.000 Test loss:  0.069 RMSE: 2.463\n",
      "Epoch: 2200, Train loss: -0.001 Test loss:  0.057 RMSE: 2.442\n",
      "Epoch: 2250, Train loss: -0.000 Test loss:  0.066 RMSE: 2.458\n",
      "Epoch: 2300, Train loss: -0.000 Test loss:  0.052 RMSE: 2.415\n",
      "Epoch: 2350, Train loss: -0.000 Test loss:  0.051 RMSE: 2.396\n",
      "Epoch: 2400, Train loss: -0.000 Test loss:  0.017 RMSE: 2.392\n",
      "Epoch: 2450, Train loss: -0.000 Test loss:  0.033 RMSE: 2.424\n",
      "Epoch: 2500, Train loss: -0.000 Test loss:  0.033 RMSE: 2.396\n",
      "Epoch: 2550, Train loss: -0.000 Test loss: -0.016 RMSE: 2.300\n",
      "Epoch: 2600, Train loss: -0.000 Test loss: -0.013 RMSE: 2.372\n",
      "Epoch: 2650, Train loss: -0.001 Test loss:  0.033 RMSE: 2.396\n",
      "Epoch: 2700, Train loss: -0.001 Test loss: -0.009 RMSE: 2.361\n",
      "Epoch: 2750, Train loss: -0.001 Test loss: -0.014 RMSE: 2.331\n",
      "Epoch: 2800, Train loss: -0.001 Test loss:  0.036 RMSE: 2.389\n",
      "Epoch: 2850, Train loss: -0.001 Test loss: -0.025 RMSE: 2.347\n",
      "Epoch: 2900, Train loss: -0.001 Test loss: -0.020 RMSE: 2.327\n",
      "Epoch: 2950, Train loss: -0.001 Test loss: -0.010 RMSE: 2.373\n",
      "Epoch: 3000, Train loss: -0.001 Test loss:  0.020 RMSE: 2.351\n",
      "Epoch: 3050, Train loss: -0.001 Test loss: -0.004 RMSE: 2.349\n",
      "Epoch: 3100, Train loss: -0.001 Test loss: -0.016 RMSE: 2.309\n",
      "Epoch: 3150, Train loss: -0.001 Test loss:  0.028 RMSE: 2.372\n",
      "Epoch: 3200, Train loss: -0.001 Test loss:  0.010 RMSE: 2.339\n",
      "Epoch: 3250, Train loss: -0.000 Test loss:  0.013 RMSE: 2.342\n",
      "Epoch: 3300, Train loss: -0.000 Test loss: -0.003 RMSE: 2.354\n",
      "Epoch: 3350, Train loss: -0.001 Test loss:  0.042 RMSE: 2.345\n",
      "Epoch: 3400, Train loss: -0.001 Test loss: -0.024 RMSE: 2.285\n",
      "Epoch: 3450, Train loss: -0.001 Test loss: -0.024 RMSE: 2.307\n",
      "Epoch: 3500, Train loss: -0.001 Test loss:  0.024 RMSE: 2.366\n",
      "Epoch: 3550, Train loss: -0.001 Test loss:  0.010 RMSE: 2.334\n",
      "Epoch: 3600, Train loss: -0.001 Test loss: -0.002 RMSE: 2.321\n",
      "Epoch: 3650, Train loss: -0.001 Test loss: -0.013 RMSE: 2.276\n",
      "Epoch: 3700, Train loss: -0.001 Test loss: -0.015 RMSE: 2.244\n",
      "Epoch: 3750, Train loss: -0.001 Test loss: -0.022 RMSE: 2.274\n",
      "Epoch: 3800, Train loss: -0.001 Test loss:  0.022 RMSE: 2.311\n",
      "Epoch: 3850, Train loss: -0.001 Test loss:  0.002 RMSE: 2.319\n",
      "Epoch: 3900, Train loss: -0.001 Test loss: -0.045 RMSE: 2.254\n",
      "Epoch: 3950, Train loss: -0.001 Test loss: -0.020 RMSE: 2.275\n",
      "Epoch: 4000, Train loss: -0.001 Test loss: -0.051 RMSE: 2.234\n",
      "Epoch: 4050, Train loss: -0.001 Test loss: -0.026 RMSE: 2.281\n",
      "Epoch: 4100, Train loss: -0.001 Test loss:  0.036 RMSE: 2.338\n",
      "Epoch: 4150, Train loss: -0.001 Test loss: -0.077 RMSE: 2.214\n",
      "Epoch: 4200, Train loss: -0.001 Test loss: -0.013 RMSE: 2.290\n",
      "Epoch: 4250, Train loss: -0.001 Test loss: -0.009 RMSE: 2.277\n",
      "Epoch: 4300, Train loss: -0.001 Test loss: -0.040 RMSE: 2.246\n",
      "Epoch: 4350, Train loss: -0.001 Test loss:  0.003 RMSE: 2.285\n",
      "Epoch: 4400, Train loss: -0.001 Test loss:  0.001 RMSE: 2.323\n",
      "Epoch: 4450, Train loss: -0.001 Test loss:  0.014 RMSE: 2.268\n",
      "Epoch: 4500, Train loss: -0.001 Test loss:  0.006 RMSE: 2.315\n",
      "Epoch: 4550, Train loss: -0.000 Test loss: -0.010 RMSE: 2.301\n",
      "Epoch: 4600, Train loss: -0.001 Test loss:  0.019 RMSE: 2.296\n",
      "Epoch: 4650, Train loss: -0.000 Test loss: -0.036 RMSE: 2.274\n",
      "Epoch: 4700, Train loss: -0.001 Test loss:  0.007 RMSE: 2.312\n",
      "Epoch: 4750, Train loss: -0.001 Test loss: -0.043 RMSE: 2.266\n",
      "Epoch: 4800, Train loss: -0.000 Test loss: -0.053 RMSE: 2.281\n",
      "Epoch: 4850, Train loss: -0.001 Test loss:  0.031 RMSE: 2.357\n",
      "Epoch: 4900, Train loss: -0.001 Test loss: -0.016 RMSE: 2.308\n",
      "Epoch: 4950, Train loss: -0.001 Test loss:  0.025 RMSE: 2.330\n",
      "Epoch: 5000, Train loss: -0.001 Test loss: -0.014 RMSE: 2.303\n",
      "Epoch: 5050, Train loss: -0.001 Test loss: -0.013 RMSE: 2.285\n",
      "Epoch: 5100, Train loss: -0.001 Test loss: -0.019 RMSE: 2.286\n",
      "Epoch: 5150, Train loss: -0.001 Test loss: -0.020 RMSE: 2.258\n",
      "Epoch: 5200, Train loss: -0.001 Test loss:  0.005 RMSE: 2.306\n",
      "Epoch: 5250, Train loss: -0.001 Test loss: -0.018 RMSE: 2.254\n",
      "Epoch: 5300, Train loss: -0.001 Test loss:  0.010 RMSE: 2.269\n",
      "Epoch: 5350, Train loss: -0.001 Test loss:  0.023 RMSE: 2.273\n",
      "Epoch: 5400, Train loss: -0.000 Test loss:  0.012 RMSE: 2.273\n",
      "Epoch: 5450, Train loss: -0.001 Test loss: -0.021 RMSE: 2.286\n",
      "Epoch: 5500, Train loss: -0.001 Test loss:  0.013 RMSE: 2.297\n",
      "Epoch: 5550, Train loss: -0.001 Test loss: -0.001 RMSE: 2.251\n",
      "Epoch: 5600, Train loss: -0.001 Test loss: -0.022 RMSE: 2.261\n",
      "Epoch: 5650, Train loss: -0.001 Test loss: -0.014 RMSE: 2.280\n",
      "Epoch: 5700, Train loss: -0.001 Test loss: -0.025 RMSE: 2.253\n",
      "Epoch: 5750, Train loss: -0.001 Test loss: -0.047 RMSE: 2.228\n",
      "Epoch: 5800, Train loss: -0.001 Test loss: -0.041 RMSE: 2.226\n",
      "Epoch: 5850, Train loss: -0.001 Test loss:  0.024 RMSE: 2.327\n",
      "Epoch: 5900, Train loss: -0.001 Test loss: -0.029 RMSE: 2.246\n",
      "Epoch: 5950, Train loss: -0.001 Test loss: -0.048 RMSE: 2.241\n",
      "Epoch: 6000, Train loss: -0.001 Test loss: -0.032 RMSE: 2.202\n",
      "Epoch: 6050, Train loss: -0.001 Test loss: -0.049 RMSE: 2.224\n",
      "Epoch: 6100, Train loss: -0.000 Test loss: -0.079 RMSE: 2.168\n",
      "Epoch: 6150, Train loss: -0.001 Test loss: -0.041 RMSE: 2.241\n",
      "Epoch: 6200, Train loss: -0.001 Test loss: -0.013 RMSE: 2.279\n",
      "Epoch: 6250, Train loss: -0.001 Test loss: -0.057 RMSE: 2.184\n",
      "Epoch: 6300, Train loss: -0.001 Test loss: -0.038 RMSE: 2.206\n",
      "Epoch: 6350, Train loss: -0.001 Test loss: -0.014 RMSE: 2.259\n",
      "Epoch: 6400, Train loss: -0.001 Test loss:  0.020 RMSE: 2.294\n",
      "Epoch: 6450, Train loss: -0.001 Test loss: -0.043 RMSE: 2.231\n",
      "Epoch: 6500, Train loss: -0.000 Test loss: -0.005 RMSE: 2.254\n",
      "Epoch: 6550, Train loss: -0.001 Test loss: -0.056 RMSE: 2.227\n",
      "Epoch: 6600, Train loss: -0.001 Test loss:  0.006 RMSE: 2.262\n",
      "Epoch: 6650, Train loss: -0.001 Test loss: -0.043 RMSE: 2.245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6700, Train loss: -0.001 Test loss: -0.008 RMSE: 2.297\n",
      "Epoch: 6750, Train loss: -0.001 Test loss:  0.004 RMSE: 2.247\n",
      "Epoch: 6800, Train loss: -0.001 Test loss: -0.042 RMSE: 2.240\n",
      "Epoch: 6850, Train loss: -0.001 Test loss:  0.016 RMSE: 2.288\n",
      "Epoch: 6900, Train loss: -0.001 Test loss:  0.035 RMSE: 2.278\n",
      "Epoch: 6950, Train loss: -0.001 Test loss: -0.021 RMSE: 2.222\n",
      "Epoch: 7000, Train loss: -0.001 Test loss: -0.062 RMSE: 2.230\n",
      "Epoch: 7050, Train loss: -0.001 Test loss:  0.009 RMSE: 2.262\n",
      "Epoch: 7100, Train loss: -0.001 Test loss: -0.043 RMSE: 2.196\n",
      "Epoch: 7150, Train loss: -0.001 Test loss: -0.036 RMSE: 2.243\n",
      "Epoch: 7200, Train loss: -0.001 Test loss: -0.016 RMSE: 2.221\n",
      "Epoch: 7250, Train loss: -0.001 Test loss: -0.011 RMSE: 2.275\n",
      "Epoch: 7300, Train loss: -0.001 Test loss: -0.009 RMSE: 2.246\n",
      "Epoch: 7350, Train loss: -0.000 Test loss: -0.023 RMSE: 2.228\n",
      "Epoch: 7400, Train loss: -0.001 Test loss: -0.060 RMSE: 2.209\n",
      "Epoch: 7450, Train loss: -0.001 Test loss: -0.060 RMSE: 2.247\n",
      "Epoch: 7500, Train loss: -0.001 Test loss: -0.013 RMSE: 2.221\n",
      "Epoch: 7550, Train loss: -0.001 Test loss: -0.041 RMSE: 2.243\n",
      "Epoch: 7600, Train loss: -0.001 Test loss: -0.048 RMSE: 2.202\n",
      "Epoch: 7650, Train loss: -0.000 Test loss:  0.021 RMSE: 2.262\n",
      "Epoch: 7700, Train loss: -0.001 Test loss: -0.040 RMSE: 2.248\n",
      "Epoch: 7750, Train loss: -0.001 Test loss: -0.019 RMSE: 2.262\n",
      "Epoch: 7800, Train loss: -0.001 Test loss: -0.003 RMSE: 2.283\n",
      "Epoch: 7850, Train loss: -0.001 Test loss: -0.029 RMSE: 2.244\n",
      "Epoch: 7900, Train loss: -0.001 Test loss:  0.021 RMSE: 2.256\n",
      "Epoch: 7950, Train loss: -0.001 Test loss: -0.061 RMSE: 2.232\n",
      "Epoch: 8000, Train loss: -0.001 Test loss: -0.037 RMSE: 2.235\n",
      "Epoch: 8050, Train loss: -0.001 Test loss: -0.044 RMSE: 2.218\n",
      "Epoch: 8100, Train loss: -0.001 Test loss: -0.003 RMSE: 2.289\n",
      "Epoch: 8150, Train loss: -0.001 Test loss: -0.054 RMSE: 2.251\n",
      "Epoch: 8200, Train loss: -0.001 Test loss: -0.029 RMSE: 2.194\n",
      "Epoch: 8250, Train loss: -0.001 Test loss: -0.032 RMSE: 2.229\n",
      "Epoch: 8300, Train loss: -0.001 Test loss: -0.039 RMSE: 2.232\n",
      "Epoch: 8350, Train loss: -0.001 Test loss: -0.015 RMSE: 2.261\n",
      "Epoch: 8400, Train loss: -0.001 Test loss: -0.027 RMSE: 2.247\n",
      "Epoch: 8450, Train loss: -0.001 Test loss: -0.044 RMSE: 2.210\n",
      "Epoch: 8500, Train loss: -0.001 Test loss: -0.027 RMSE: 2.268\n",
      "Epoch: 8550, Train loss: -0.001 Test loss: -0.040 RMSE: 2.187\n",
      "Epoch: 8600, Train loss: -0.001 Test loss:  0.017 RMSE: 2.286\n",
      "Epoch: 8650, Train loss: -0.001 Test loss: -0.029 RMSE: 2.184\n",
      "Epoch: 8700, Train loss: -0.001 Test loss: -0.025 RMSE: 2.192\n",
      "Epoch: 8750, Train loss: -0.001 Test loss:  0.004 RMSE: 2.225\n",
      "Epoch: 8800, Train loss: -0.001 Test loss: -0.051 RMSE: 2.174\n",
      "Epoch: 8850, Train loss: -0.001 Test loss: -0.034 RMSE: 2.212\n",
      "Epoch: 8900, Train loss: -0.001 Test loss: -0.029 RMSE: 2.244\n",
      "Epoch: 8950, Train loss: -0.001 Test loss: -0.023 RMSE: 2.209\n",
      "Epoch: 9000, Train loss: -0.001 Test loss: -0.043 RMSE: 2.195\n",
      "Epoch: 9050, Train loss: -0.001 Test loss: -0.055 RMSE: 2.229\n",
      "Epoch: 9100, Train loss: -0.001 Test loss: -0.033 RMSE: 2.198\n",
      "Epoch: 9150, Train loss: -0.001 Test loss: -0.043 RMSE: 2.234\n",
      "Epoch: 9200, Train loss: -0.001 Test loss:  0.001 RMSE: 2.318\n",
      "Epoch: 9250, Train loss: -0.001 Test loss: -0.074 RMSE: 2.149\n",
      "Epoch: 9300, Train loss: -0.001 Test loss: -0.032 RMSE: 2.203\n",
      "Epoch: 9350, Train loss: -0.001 Test loss: -0.042 RMSE: 2.194\n",
      "Epoch: 9400, Train loss: -0.001 Test loss: -0.022 RMSE: 2.275\n",
      "Epoch: 9450, Train loss: -0.001 Test loss:  0.021 RMSE: 2.274\n",
      "Epoch: 9500, Train loss: -0.001 Test loss:  0.007 RMSE: 2.261\n",
      "Epoch: 9550, Train loss: -0.001 Test loss: -0.011 RMSE: 2.290\n",
      "Epoch: 9600, Train loss: -0.000 Test loss: -0.027 RMSE: 2.248\n",
      "Epoch: 9650, Train loss: -0.001 Test loss:  0.010 RMSE: 2.246\n",
      "Epoch: 9700, Train loss: -0.001 Test loss: -0.051 RMSE: 2.238\n",
      "Epoch: 9750, Train loss: -0.001 Test loss: -0.021 RMSE: 2.244\n",
      "Epoch: 9800, Train loss: -0.001 Test loss: -0.029 RMSE: 2.198\n",
      "Epoch: 9850, Train loss: -0.001 Test loss: -0.041 RMSE: 2.194\n",
      "Epoch: 9900, Train loss: -0.001 Test loss: -0.037 RMSE: 2.189\n",
      "Epoch: 9950, Train loss: -0.001 Test loss: -0.022 RMSE: 2.255\n",
      "Epoch: 10000, Train loss: -0.001 Test loss: -0.056 RMSE: 2.194\n",
      "Epoch: 10050, Train loss: -0.001 Test loss: -0.002 RMSE: 2.288\n",
      "Epoch: 10100, Train loss: -0.001 Test loss: -0.002 RMSE: 2.224\n",
      "Epoch: 10150, Train loss: -0.001 Test loss: -0.056 RMSE: 2.209\n",
      "Epoch: 10200, Train loss: -0.001 Test loss: -0.025 RMSE: 2.218\n",
      "Epoch: 10250, Train loss: -0.001 Test loss: -0.031 RMSE: 2.282\n",
      "Epoch: 10300, Train loss: -0.001 Test loss:  0.004 RMSE: 2.289\n",
      "Epoch: 10350, Train loss: -0.001 Test loss:  0.019 RMSE: 2.280\n",
      "Epoch: 10400, Train loss: -0.001 Test loss: -0.030 RMSE: 2.213\n",
      "Epoch: 10450, Train loss: -0.001 Test loss:  0.057 RMSE: 2.315\n",
      "Epoch: 10500, Train loss: -0.001 Test loss: -0.014 RMSE: 2.251\n",
      "Epoch: 10550, Train loss: -0.001 Test loss: -0.059 RMSE: 2.177\n",
      "Epoch: 10600, Train loss: -0.001 Test loss: -0.010 RMSE: 2.268\n",
      "Epoch: 10650, Train loss: -0.001 Test loss: -0.032 RMSE: 2.272\n",
      "Epoch: 10700, Train loss: -0.001 Test loss: -0.011 RMSE: 2.238\n",
      "Epoch: 10750, Train loss: -0.001 Test loss:  0.027 RMSE: 2.241\n",
      "Epoch: 10800, Train loss: -0.001 Test loss: -0.017 RMSE: 2.256\n",
      "Epoch: 10850, Train loss: -0.001 Test loss: -0.045 RMSE: 2.194\n",
      "Epoch: 10900, Train loss: -0.001 Test loss: -0.026 RMSE: 2.202\n",
      "Epoch: 10950, Train loss: -0.001 Test loss: -0.013 RMSE: 2.289\n",
      "Epoch: 11000, Train loss: -0.001 Test loss: -0.034 RMSE: 2.253\n",
      "Epoch: 11050, Train loss: -0.000 Test loss: -0.061 RMSE: 2.176\n",
      "Epoch: 11100, Train loss: -0.001 Test loss: -0.040 RMSE: 2.266\n",
      "Epoch: 11150, Train loss: -0.001 Test loss: -0.011 RMSE: 2.268\n",
      "Epoch: 11200, Train loss: -0.001 Test loss: -0.056 RMSE: 2.219\n",
      "Epoch: 11250, Train loss: -0.001 Test loss: -0.024 RMSE: 2.277\n",
      "Epoch: 11300, Train loss: -0.001 Test loss: -0.047 RMSE: 2.230\n",
      "Epoch: 11350, Train loss: -0.001 Test loss: -0.039 RMSE: 2.276\n",
      "Epoch: 11400, Train loss: -0.001 Test loss: -0.000 RMSE: 2.259\n",
      "Epoch: 11450, Train loss: -0.001 Test loss:  0.000 RMSE: 2.270\n",
      "Epoch: 11500, Train loss: -0.001 Test loss: -0.002 RMSE: 2.263\n",
      "Epoch: 11550, Train loss: -0.001 Test loss: -0.055 RMSE: 2.252\n",
      "Epoch: 11600, Train loss: -0.001 Test loss: -0.012 RMSE: 2.261\n",
      "Epoch: 11650, Train loss: -0.001 Test loss: -0.039 RMSE: 2.210\n",
      "Epoch: 11700, Train loss: -0.001 Test loss:  0.016 RMSE: 2.250\n",
      "Epoch: 11750, Train loss: -0.001 Test loss: -0.001 RMSE: 2.275\n",
      "Epoch: 11800, Train loss: -0.001 Test loss:  0.030 RMSE: 2.282\n",
      "Epoch: 11850, Train loss: -0.001 Test loss: -0.034 RMSE: 2.207\n",
      "Epoch: 11900, Train loss: -0.001 Test loss: -0.004 RMSE: 2.266\n",
      "Epoch: 11950, Train loss: -0.001 Test loss: -0.019 RMSE: 2.250\n",
      "Epoch: 12000, Train loss: -0.001 Test loss:  0.002 RMSE: 2.261\n",
      "Epoch: 12050, Train loss: -0.001 Test loss:  0.004 RMSE: 2.290\n",
      "Epoch: 12100, Train loss: -0.001 Test loss: -0.049 RMSE: 2.223\n",
      "Epoch: 12150, Train loss: -0.001 Test loss: -0.002 RMSE: 2.235\n",
      "Epoch: 12200, Train loss: -0.001 Test loss: -0.038 RMSE: 2.208\n",
      "Epoch: 12250, Train loss: -0.001 Test loss: -0.052 RMSE: 2.195\n",
      "Epoch: 12300, Train loss: -0.000 Test loss: -0.030 RMSE: 2.239\n",
      "Epoch: 12350, Train loss: -0.001 Test loss: -0.006 RMSE: 2.253\n",
      "Epoch: 12400, Train loss: -0.001 Test loss: -0.041 RMSE: 2.219\n",
      "Epoch: 12450, Train loss: -0.000 Test loss:  0.001 RMSE: 2.230\n",
      "Epoch: 12500, Train loss: -0.001 Test loss: -0.030 RMSE: 2.240\n",
      "Epoch: 12550, Train loss: -0.001 Test loss: -0.044 RMSE: 2.225\n",
      "Epoch: 12600, Train loss: -0.001 Test loss: -0.011 RMSE: 2.240\n",
      "Epoch: 12650, Train loss: -0.001 Test loss: -0.016 RMSE: 2.246\n",
      "Epoch: 12700, Train loss: -0.001 Test loss: -0.039 RMSE: 2.242\n",
      "Epoch: 12750, Train loss: -0.001 Test loss: -0.010 RMSE: 2.230\n",
      "Epoch: 12800, Train loss: -0.001 Test loss: -0.044 RMSE: 2.231\n",
      "Epoch: 12850, Train loss: -0.001 Test loss:  0.009 RMSE: 2.291\n",
      "Epoch: 12900, Train loss: -0.001 Test loss: -0.036 RMSE: 2.229\n",
      "Epoch: 12950, Train loss: -0.001 Test loss: -0.010 RMSE: 2.270\n",
      "Epoch: 13000, Train loss: -0.001 Test loss: -0.034 RMSE: 2.246\n",
      "Epoch: 13050, Train loss: -0.001 Test loss: -0.030 RMSE: 2.202\n",
      "Epoch: 13100, Train loss: -0.001 Test loss: -0.036 RMSE: 2.219\n",
      "Epoch: 13150, Train loss: -0.000 Test loss: -0.007 RMSE: 2.256\n",
      "Epoch: 13200, Train loss: -0.001 Test loss: -0.001 RMSE: 2.246\n",
      "Epoch: 13250, Train loss: -0.001 Test loss: -0.070 RMSE: 2.200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13300, Train loss: -0.001 Test loss: -0.012 RMSE: 2.235\n",
      "Epoch: 13350, Train loss: -0.001 Test loss:  0.008 RMSE: 2.267\n",
      "Epoch: 13400, Train loss: -0.001 Test loss: -0.063 RMSE: 2.235\n",
      "Epoch: 13450, Train loss: -0.001 Test loss: -0.033 RMSE: 2.211\n",
      "Epoch: 13500, Train loss: -0.001 Test loss: -0.054 RMSE: 2.245\n",
      "Epoch: 13550, Train loss: -0.001 Test loss: -0.068 RMSE: 2.215\n",
      "Epoch: 13600, Train loss: -0.001 Test loss:  0.002 RMSE: 2.204\n",
      "Epoch: 13650, Train loss: -0.001 Test loss:  0.010 RMSE: 2.297\n",
      "Epoch: 13700, Train loss: -0.001 Test loss:  0.002 RMSE: 2.255\n",
      "Epoch: 13750, Train loss: -0.001 Test loss: -0.007 RMSE: 2.216\n",
      "Epoch: 13800, Train loss: -0.001 Test loss: -0.031 RMSE: 2.210\n",
      "Epoch: 13850, Train loss: -0.001 Test loss: -0.021 RMSE: 2.217\n",
      "Epoch: 13900, Train loss: -0.001 Test loss: -0.043 RMSE: 2.223\n",
      "Epoch: 13950, Train loss: -0.001 Test loss: -0.004 RMSE: 2.237\n",
      "Epoch: 14000, Train loss: -0.001 Test loss: -0.035 RMSE: 2.202\n",
      "Epoch: 14050, Train loss: -0.001 Test loss:  0.003 RMSE: 2.267\n",
      "Epoch: 14100, Train loss: -0.001 Test loss: -0.020 RMSE: 2.225\n",
      "Epoch: 14150, Train loss: -0.001 Test loss: -0.039 RMSE: 2.198\n",
      "Epoch: 14200, Train loss: -0.001 Test loss: -0.048 RMSE: 2.246\n",
      "Epoch: 14250, Train loss: -0.001 Test loss: -0.006 RMSE: 2.222\n",
      "Epoch: 14300, Train loss: -0.001 Test loss: -0.034 RMSE: 2.238\n",
      "Epoch: 14350, Train loss: -0.001 Test loss: -0.013 RMSE: 2.267\n",
      "Epoch: 14400, Train loss: -0.001 Test loss: -0.016 RMSE: 2.207\n",
      "Epoch: 14450, Train loss: -0.001 Test loss: -0.039 RMSE: 2.215\n",
      "Epoch: 14500, Train loss: -0.001 Test loss: -0.036 RMSE: 2.188\n",
      "Epoch: 14550, Train loss: -0.001 Test loss: -0.040 RMSE: 2.220\n",
      "Epoch: 14600, Train loss: -0.001 Test loss: -0.030 RMSE: 2.201\n",
      "Epoch: 14650, Train loss: -0.001 Test loss:  0.014 RMSE: 2.247\n",
      "Epoch: 14700, Train loss: -0.001 Test loss:  0.020 RMSE: 2.300\n",
      "Epoch: 14750, Train loss: -0.001 Test loss: -0.022 RMSE: 2.223\n",
      "Epoch: 14800, Train loss: -0.001 Test loss: -0.025 RMSE: 2.266\n",
      "Epoch: 14850, Train loss: -0.001 Test loss:  0.012 RMSE: 2.246\n",
      "Epoch: 14900, Train loss: -0.001 Test loss: -0.037 RMSE: 2.199\n",
      "Epoch: 14950, Train loss: -0.001 Test loss: -0.097 RMSE: 2.213\n",
      "Epoch: 15000, Train loss: -0.001 Test loss: -0.020 RMSE: 2.260\n",
      "Epoch: 15050, Train loss: -0.001 Test loss: -0.005 RMSE: 2.273\n",
      "Epoch: 15100, Train loss: -0.001 Test loss: -0.028 RMSE: 2.219\n",
      "Epoch: 15150, Train loss: -0.001 Test loss: -0.008 RMSE: 2.253\n",
      "Epoch: 15200, Train loss: -0.000 Test loss: -0.030 RMSE: 2.228\n",
      "Epoch: 15250, Train loss: -0.001 Test loss: -0.022 RMSE: 2.238\n",
      "Epoch: 15300, Train loss: -0.001 Test loss: -0.001 RMSE: 2.205\n",
      "Epoch: 15350, Train loss: -0.001 Test loss: -0.051 RMSE: 2.254\n",
      "Epoch: 15400, Train loss: -0.001 Test loss:  0.012 RMSE: 2.278\n",
      "Epoch: 15450, Train loss: -0.001 Test loss:  0.023 RMSE: 2.272\n",
      "Epoch: 15500, Train loss: -0.001 Test loss: -0.051 RMSE: 2.227\n",
      "Epoch: 15550, Train loss: -0.001 Test loss: -0.007 RMSE: 2.267\n",
      "Epoch: 15600, Train loss: -0.001 Test loss: -0.027 RMSE: 2.287\n",
      "Epoch: 15650, Train loss: -0.001 Test loss: -0.027 RMSE: 2.247\n",
      "Epoch: 15700, Train loss: -0.001 Test loss:  0.051 RMSE: 2.298\n",
      "Epoch: 15750, Train loss: -0.001 Test loss: -0.018 RMSE: 2.229\n",
      "Epoch: 15800, Train loss: -0.001 Test loss: -0.029 RMSE: 2.260\n",
      "Epoch: 15850, Train loss: -0.001 Test loss:  0.027 RMSE: 2.295\n",
      "Epoch: 15900, Train loss: -0.001 Test loss:  0.002 RMSE: 2.259\n",
      "Epoch: 15950, Train loss: -0.001 Test loss: -0.070 RMSE: 2.220\n",
      "Epoch: 16000, Train loss: -0.001 Test loss: -0.005 RMSE: 2.235\n",
      "Epoch: 16050, Train loss: -0.001 Test loss: -0.015 RMSE: 2.283\n",
      "Epoch: 16100, Train loss: -0.001 Test loss: -0.038 RMSE: 2.198\n",
      "Epoch: 16150, Train loss: -0.001 Test loss: -0.044 RMSE: 2.230\n",
      "Epoch: 16200, Train loss: -0.001 Test loss:  0.020 RMSE: 2.272\n",
      "Epoch: 16250, Train loss: -0.001 Test loss:  0.016 RMSE: 2.280\n",
      "Epoch: 16300, Train loss: -0.001 Test loss: -0.017 RMSE: 2.268\n",
      "Epoch: 16350, Train loss: -0.001 Test loss: -0.010 RMSE: 2.246\n",
      "Epoch: 16400, Train loss: -0.001 Test loss: -0.015 RMSE: 2.285\n",
      "Epoch: 16450, Train loss: -0.001 Test loss:  0.028 RMSE: 2.281\n",
      "Epoch: 16500, Train loss: -0.001 Test loss: -0.031 RMSE: 2.218\n",
      "Epoch: 16550, Train loss: -0.001 Test loss: -0.045 RMSE: 2.215\n",
      "Epoch: 16600, Train loss: -0.001 Test loss:  0.002 RMSE: 2.265\n",
      "Epoch: 16650, Train loss: -0.001 Test loss: -0.008 RMSE: 2.255\n",
      "Epoch: 16700, Train loss: -0.001 Test loss: -0.037 RMSE: 2.256\n",
      "Epoch: 16750, Train loss: -0.001 Test loss: -0.042 RMSE: 2.244\n",
      "Epoch: 16800, Train loss: -0.001 Test loss:  0.011 RMSE: 2.265\n",
      "Epoch: 16850, Train loss: -0.001 Test loss: -0.018 RMSE: 2.230\n",
      "Epoch: 16900, Train loss: -0.001 Test loss: -0.051 RMSE: 2.215\n",
      "Epoch: 16950, Train loss: -0.001 Test loss: -0.044 RMSE: 2.222\n",
      "Epoch: 17000, Train loss: -0.001 Test loss:  0.007 RMSE: 2.256\n",
      "Epoch: 17050, Train loss: -0.001 Test loss: -0.023 RMSE: 2.235\n",
      "Epoch: 17100, Train loss: -0.001 Test loss: -0.016 RMSE: 2.310\n",
      "Epoch: 17150, Train loss: -0.001 Test loss: -0.037 RMSE: 2.227\n",
      "Epoch: 17200, Train loss: -0.001 Test loss: -0.009 RMSE: 2.240\n",
      "Epoch: 17250, Train loss: -0.001 Test loss: -0.017 RMSE: 2.215\n",
      "Epoch: 17300, Train loss: -0.001 Test loss:  0.009 RMSE: 2.299\n",
      "Epoch: 17350, Train loss: -0.001 Test loss: -0.055 RMSE: 2.201\n",
      "Epoch: 17400, Train loss: -0.001 Test loss: -0.004 RMSE: 2.257\n",
      "Epoch: 17450, Train loss: -0.001 Test loss:  0.006 RMSE: 2.290\n",
      "Epoch: 17500, Train loss: -0.001 Test loss:  0.009 RMSE: 2.244\n",
      "Epoch: 17550, Train loss: -0.001 Test loss: -0.035 RMSE: 2.222\n",
      "Epoch: 17600, Train loss: -0.001 Test loss: -0.009 RMSE: 2.294\n",
      "Epoch: 17650, Train loss: -0.001 Test loss:  0.006 RMSE: 2.267\n",
      "Epoch: 17700, Train loss: -0.001 Test loss:  0.043 RMSE: 2.297\n",
      "Epoch: 17750, Train loss: -0.001 Test loss: -0.043 RMSE: 2.263\n",
      "Epoch: 17800, Train loss: -0.001 Test loss: -0.028 RMSE: 2.270\n",
      "Epoch: 17850, Train loss: -0.001 Test loss:  0.023 RMSE: 2.283\n",
      "Epoch: 17900, Train loss: -0.001 Test loss: -0.021 RMSE: 2.231\n",
      "Epoch: 17950, Train loss: -0.001 Test loss:  0.009 RMSE: 2.253\n",
      "Epoch: 18000, Train loss: -0.001 Test loss: -0.016 RMSE: 2.225\n",
      "Epoch: 18050, Train loss: -0.001 Test loss:  0.006 RMSE: 2.259\n",
      "Epoch: 18100, Train loss: -0.001 Test loss:  0.006 RMSE: 2.264\n",
      "Epoch: 18150, Train loss: -0.001 Test loss: -0.002 RMSE: 2.238\n",
      "Epoch: 18200, Train loss: -0.001 Test loss:  0.035 RMSE: 2.272\n",
      "Epoch: 18250, Train loss: -0.001 Test loss:  0.028 RMSE: 2.245\n",
      "Epoch: 18300, Train loss: -0.001 Test loss: -0.008 RMSE: 2.251\n",
      "Epoch: 18350, Train loss: -0.001 Test loss: -0.060 RMSE: 2.224\n",
      "Epoch: 18400, Train loss: -0.001 Test loss:  0.021 RMSE: 2.291\n",
      "Epoch: 18450, Train loss: -0.001 Test loss: -0.030 RMSE: 2.281\n",
      "Epoch: 18500, Train loss: -0.001 Test loss: -0.018 RMSE: 2.235\n",
      "Epoch: 18550, Train loss: -0.001 Test loss: -0.003 RMSE: 2.254\n",
      "Epoch: 18600, Train loss: -0.001 Test loss: -0.012 RMSE: 2.268\n",
      "Epoch: 18650, Train loss: -0.001 Test loss:  0.017 RMSE: 2.216\n",
      "Epoch: 18700, Train loss: -0.001 Test loss: -0.053 RMSE: 2.216\n",
      "Epoch: 18750, Train loss: -0.001 Test loss: -0.033 RMSE: 2.225\n",
      "Epoch: 18800, Train loss: -0.001 Test loss: -0.039 RMSE: 2.215\n",
      "Epoch: 18850, Train loss: -0.001 Test loss: -0.055 RMSE: 2.223\n",
      "Epoch: 18900, Train loss: -0.001 Test loss: -0.035 RMSE: 2.206\n",
      "Epoch: 18950, Train loss: -0.001 Test loss:  0.001 RMSE: 2.202\n",
      "Epoch: 19000, Train loss: -0.001 Test loss: -0.041 RMSE: 2.216\n",
      "Epoch: 19050, Train loss: -0.001 Test loss:  0.026 RMSE: 2.228\n",
      "Epoch: 19100, Train loss: -0.001 Test loss: -0.070 RMSE: 2.219\n",
      "Epoch: 19150, Train loss: -0.001 Test loss: -0.068 RMSE: 2.176\n",
      "Epoch: 19200, Train loss: -0.001 Test loss: -0.083 RMSE: 2.200\n",
      "Epoch: 19250, Train loss: -0.001 Test loss: -0.021 RMSE: 2.244\n",
      "Epoch: 19300, Train loss: -0.000 Test loss: -0.036 RMSE: 2.240\n",
      "Epoch: 19350, Train loss: -0.001 Test loss: -0.069 RMSE: 2.193\n",
      "Epoch: 19400, Train loss: -0.001 Test loss: -0.077 RMSE: 2.194\n",
      "Epoch: 19450, Train loss: -0.001 Test loss:  0.020 RMSE: 2.303\n",
      "Epoch: 19500, Train loss: -0.001 Test loss: -0.022 RMSE: 2.200\n",
      "Epoch: 19550, Train loss: -0.001 Test loss: -0.048 RMSE: 2.212\n",
      "Epoch: 19600, Train loss: -0.001 Test loss: -0.013 RMSE: 2.219\n",
      "Epoch: 19650, Train loss: -0.001 Test loss: -0.037 RMSE: 2.221\n",
      "Epoch: 19700, Train loss: -0.001 Test loss: -0.036 RMSE: 2.210\n",
      "Epoch: 19750, Train loss: -0.001 Test loss: -0.040 RMSE: 2.230\n",
      "Epoch: 19800, Train loss: -0.001 Test loss: -0.022 RMSE: 2.182\n",
      "Epoch: 19850, Train loss: -0.001 Test loss: -0.015 RMSE: 2.239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19900, Train loss: -0.001 Test loss: -0.036 RMSE: 2.201\n",
      "Epoch: 19950, Train loss: -0.001 Test loss: -0.041 RMSE: 2.221\n",
      "Epoch: 19999, Train loss: -0.001 Test loss: -0.048 RMSE: 2.216\n",
      "Train log. lik. = -1.741 +/-  0.000\n",
      "Test  log. lik. = -2.156 +/-  0.000\n",
      "Train RMSE      =  1.221 +/-  0.000\n",
      "Test  RMSE      =  2.167 +/-  0.000\n"
     ]
    }
   ],
   "source": [
    "net , losses = train_mc_dropout(x_train=x_train,y_train=y_train, x_test=x_test,y_test=y_test, y_stds=std_y_train,drop_prob=drop_prob, num_epochs=num_epochs,  num_units=layerwidth, learn_rate=learn_rate,\n",
    "                       weight_decay=weight_decay, num_samples=num_samples, log_every=log_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4e59582760>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.arange(0,len(losses),1),torch.as_tensor(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on test #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RMSE': (2.231454610824585, 3.288661479949951),\n",
       " 'LPP': (-2.170377016067505, 1.2381031513214111),\n",
       " 'gLPP': (-2.296548843383789, nan),\n",
       " 'WAIC': -3.448030114173889,\n",
       " 'PICP': 0.9411764740943909,\n",
       " 'MPIW': 7.059555530548096}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_predictors=num_samples\n",
    "samples = []\n",
    "noises = []\n",
    "for i in range(nb_predictors):\n",
    "    preds = net.network.forward(x_test).cpu().data.numpy()\n",
    "    samples.append(preds)\n",
    "    \n",
    "samples = np.array(samples)\n",
    "\n",
    "means = torch.Tensor(samples.mean(axis = 0)).view(1,-1,1)\n",
    "\n",
    "aleatoric = torch.exp(net.network.log_noise).detach()\n",
    "epistemic = torch.Tensor(samples.var(axis = 0)**0.5).view(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = means + epistemic * torch.randn(1000,len(x_test),1) \n",
    "\n",
    "\n",
    "sigma_noise = aleatoric.view(1,-1,1)\n",
    "\n",
    "evaluate_metrics(y_pred,sigma_noise.cpu(), y_test, std_y_train.squeeze(), device='cpu', std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 1]) torch.Size([1000, 51, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 51, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-2.1706185340881348, 1.2357640266418457)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Tools import log_norm\n",
    "print(y_test.shape,y_pred.shape, sigma_noise.shape)\n",
    "def LPP_Gaussian(y_pred_, y_test_, sigma_noise, y_scale):\n",
    "    y_sigma=torch.sqrt(y_pred_.var(0)+sigma_noise**2)\n",
    "    y_mean=y_pred_.mean(0)\n",
    "    LPP = log_norm(y_test_.unsqueeze(1), y_mean.unsqueeze(0), y_sigma.view(1,-1,1))\n",
    "    #account for data scaling\n",
    "    print(LPP.shape)\n",
    "    LPP-=y_scale.log() \n",
    "    MLPP = torch.mean(LPP).item()\n",
    "    SLPP = torch.std(LPP).item()\n",
    "    return (MLPP, SLPP)\n",
    "\n",
    "LPP_Gaussian(y_pred, y_test.cpu(), sigma_noise.cpu(), std_y_train.squeeze().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation of 1D normal distribution on tensors\n",
    "\n",
    "    Parameters:\n",
    "        x (Tensor): Data tensor of size S x N x 1 \n",
    "        mu (Tensor): Mean tensor of size B x S x 1\n",
    "        std (Float): Tensor of size B x S x 1(standard deviation)\n",
    "\n",
    "    Returns:\n",
    "        logproba (Tensor): size B x S x N x 1 with logproba[b,s,n]=[log p(x(s,n)|mu(b,s),std[b])]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
